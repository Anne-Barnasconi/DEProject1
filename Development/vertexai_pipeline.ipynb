{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseball win percentage prediction  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the packages\n",
    "! pip3 install --user --no-cache-dir --upgrade \"kfp>2\" \"google-cloud-pipeline-components>2\" \\\n",
    "                                        google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart the kernel\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! pip3 freeze | grep aiplatform\n",
    "! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Google Cloud project that this pipeline runs in.\n",
    "PROJECT_ID = \"data-engineering-435508\"\n",
    "# The region that this pipeline runs in\n",
    "REGION = \"us-central1\"\n",
    "# Specify a Cloud Storage URI that your pipelines service account can access. The artifacts of your pipeline runs are stored within the pipeline root.\n",
    "PIPELINE_ROOT = \"gs://temp_de2024_trs\"\n",
    "\n",
    "\n",
    "# The model_repo is specified in the pipeline definition\n",
    "# Threshold values are specified in the pipelien definition\n",
    "# Data url is specified in the pipeline definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Artifact' from 'kfp.dsl' (c:\\Users\\thomz\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\kfp\\dsl\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NamedTuple\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dsl\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdsl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (Artifact,\n\u001b[0;32m      7\u001b[0m                         Dataset,\n\u001b[0;32m      8\u001b[0m                         Input,\n\u001b[0;32m      9\u001b[0m                         Model,\n\u001b[0;32m     10\u001b[0m                         Output,\n\u001b[0;32m     11\u001b[0m                         Metrics,\n\u001b[0;32m     12\u001b[0m                         ClassificationMetrics,\n\u001b[0;32m     13\u001b[0m                         component, \n\u001b[0;32m     14\u001b[0m                         OutputPath, \n\u001b[0;32m     15\u001b[0m                         InputPath)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01maip\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle_cloud_pipeline_components\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m artifact_types\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Artifact' from 'kfp.dsl' (c:\\Users\\thomz\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\kfp\\dsl\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "import typing\n",
    "from typing import Dict\n",
    "from typing import NamedTuple\n",
    "from kfp import dsl\n",
    "from kfp.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Model,\n",
    "                        Output,\n",
    "                        Metrics,\n",
    "                        ClassificationMetrics,\n",
    "                        component, \n",
    "                        OutputPath, \n",
    "                        InputPath)\n",
    "import google.cloud.aiplatform as aip\n",
    "from google_cloud_pipeline_components.types import artifact_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pipeline components\n",
    "Components include:\n",
    "* Load data\n",
    "* Train model\n",
    "* Evaluate model\n",
    "* Deploy model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pipeline components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\",\"google-cloud-storage\"],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def download_data(project_id: str, bucket: str, file_name: str, dataset: Output[Dataset]):\n",
    "    '''download data'''\n",
    "    from google.cloud import storage\n",
    "    import pandas as pd\n",
    "    import logging \n",
    "    import sys\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    # Downloaing the file from a google bucket \n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.bucket(bucket)\n",
    "    blob = bucket.blob(file_name)\n",
    "    blob.download_to_filename(dataset.path + \".csv\")\n",
    "    logging.info('Downloaded Data!')\n",
    "    \n",
    "    \n",
    "# CURRENTLY NOT USED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dsl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;129m@dsl\u001b[39m\u001b[38;5;241m.\u001b[39mcomponent(\n\u001b[0;32m      2\u001b[0m     packages_to_install\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscikit-learn==1.3.2\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      3\u001b[0m     base_image\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython:3.10.7-slim\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m )\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_test_split_scale\u001b[39m(dataset: Input[Dataset], dataset_X_train: Output[Dataset], dataset_X_test: Output[Dataset], dataset_y_train: Output[Dataset], dataset_y_test: Output[Dataset]):\n\u001b[0;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''train_test_split and feature scaling'''\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dsl' is not defined"
     ]
    }
   ],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn==1.3.2\"],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def train_test_split(\n",
    "    dataset: Input[Dataset], \n",
    "    dataset_train: Output[Dataset], \n",
    "    dataset_test: Output[Dataset]):\n",
    "    \n",
    "    '''train_test_split'''\n",
    "    import pandas as pd\n",
    "    import logging \n",
    "    import sys\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO) \n",
    "    \n",
    "    # Load data from csv\n",
    "    alldata = pd.read_csv(dataset.path, index_col=None)\n",
    "    \n",
    "    # Split\n",
    "    train, test = train_test_split(alldata, test_size=0.3)\n",
    "    \n",
    "    # Output definitions\n",
    "    train.to_csv(dataset_train.path + \".csv\" , index=False, encoding='utf-8-sig')\n",
    "    test.to_csv(dataset_test.path + \".csv\" , index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the KNN Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=['pandas', 'scikit-learn==1.3.2'],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def train_model(\n",
    "    features: Input[Dataset], \n",
    "    model: Output[Model]):\n",
    "    \n",
    "    '''train a KNN regressor with default parameters (k = 4)'''\n",
    "    import pandas as pd\n",
    "    from sklearn.neighbors import KNeighborsRegressor\n",
    "    from sklearn import metrics\n",
    "    import json\n",
    "    import logging \n",
    "    import sys\n",
    "    import os\n",
    "    import pickle\n",
    "       \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    # Load the train set\n",
    "    data = pd.read_csv(features.path+\".csv\")\n",
    "  \n",
    "    # Train the model\n",
    "    model_knn = KNeighborsRegressor(n_neighbors=4)\n",
    "    model_knn.fit(data.drop('Win_percentage', axis = 1), data['Win_percentage'])\n",
    "    \n",
    "    # Save the model\n",
    "    model.metadata[\"framework\"] = \"KNN\"\n",
    "    file_name = model.path + f\".pkl\"\n",
    "    with open(file_name, 'wb') as file:\n",
    "        pickle.dump(model_knn, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "component() got an unexpected keyword argument 'packages_to_install'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;129m@dsl\u001b[39m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomponent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpackages_to_install\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpandas\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscikit-learn==1.3.2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnumpy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpython:3.10.7-slim\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m      4\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_model\u001b[39m(\n\u001b[0;32m      6\u001b[0m     model_knn: Input[Model], \n\u001b[0;32m      7\u001b[0m     test_set: Input[Dataset], \n\u001b[0;32m      8\u001b[0m     thresholds_dict_str: \u001b[38;5;28mstr\u001b[39m, \n\u001b[0;32m      9\u001b[0m     kpi: Output(Metrics)\n\u001b[0;32m     10\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m'\u001b[39m, approval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m):\n\u001b[0;32m     12\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''evaluate the KNN regressor'''\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: component() got an unexpected keyword argument 'packages_to_install'"
     ]
    }
   ],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=['pandas', 'scikit-learn==1.3.2','numpy'],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def evaluate_model(\n",
    "    model_knn: Input[Model], \n",
    "    test_set: Input[Dataset], \n",
    "    thresholds_dict_str: str, \n",
    "    kpi: Output[Metrics]\n",
    ") -> NamedTuple('outputs', approval=bool):\n",
    "    \n",
    "    '''evaluate the KNN regressor'''\n",
    "    import pandas as pd\n",
    "    from sklearn.neighbors import KNeighborsRegressor\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from numpy import nan_to_num\n",
    "    import json\n",
    "    import logging \n",
    "    import sys\n",
    "    import os\n",
    "    import pickle\n",
    "       \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    # Create threshold_check function\n",
    "    def threshold_check(val1, val2):\n",
    "        cond = False\n",
    "        if val1 >= val2 :\n",
    "            cond = True\n",
    "        return cond\n",
    "    \n",
    "    # Load the model\n",
    "    m_filename = model_knn.path + \".pkl\"\n",
    "    model = pickle.load(open(m_filename, 'rb'))\n",
    "    \n",
    "    # Load data \n",
    "    data = pd.read_csv(test_set.path+\".csv\")\n",
    "    X_test = data.drop(columns=[\"Win_percentage\"])\n",
    "    y_test = data['Win_percentage']\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Compute R^2\n",
    "    r_squared = model.score(X_test, y_test)\n",
    "\n",
    "    # Threshold\n",
    "    thresholds = nan_to_num(thresholds)\n",
    "    \n",
    "    thresholds_dict = json.loads(thresholds_dict_str)\n",
    "    model_knn.metadata[\"R-squared\"] = float(r_squared)\n",
    "    kpi.log_metric(\"R-squared\", float(r_squared))\n",
    "    outputs = NamedTuple('outputs', approval=bool)\n",
    "    approval_value = threshold_check(float(r_squared), int(thresholds_dict['R-squared']))\n",
    "    return outputs(approval_value)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"google-cloud-storage\"],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def upload_model_to_gcs(project_id: str, model_repo: str, model_name:str, model: Input[Model]):\n",
    "    '''upload model to gsc'''\n",
    "    from google.cloud import storage   \n",
    "    import logging \n",
    "    import sys\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)    \n",
    "  \n",
    "    # upload the model to GCS\n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.bucket(model_repo)\n",
    "    dest_file_name= model_name + '.pkl'\n",
    "    blob = bucket.blob(dest_file_name)\n",
    "    source_file_name= model.path + '.pkl'\n",
    "   \n",
    "    blob.upload_from_filename(source_file_name)    \n",
    "    \n",
    "    print(f\"File {source_file_name} uploaded to {model_repo}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "parameter without a default follows parameter with a default (3547794537.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 13\u001b[1;36m\u001b[0m\n\u001b[1;33m    thresholds_dict_str: str,\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m parameter without a default follows parameter with a default\n"
     ]
    }
   ],
   "source": [
    "@dsl.pipeline(\n",
    "    # Default pipeline root. You can override it when submitting the pipeline.\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    # A name for the pipeline. Use to determine the pipeline Context.\n",
    "    name=\"pipeline-moneyball\",\n",
    "    \n",
    ")\n",
    "def pipeline(project_id: str, data_bucket: str, dataset_uri: str, model_repo: str, thresholds_dict_str:str, model_repo_uri:str):    \n",
    "    \n",
    "    dataset_op = kfp.dsl.importer(\n",
    "        artifact_uri=dataset_uri,\n",
    "        artifact_class=Dataset,\n",
    "        reimport=False,\n",
    "    )\n",
    "    \n",
    "    # Splitting and scaling the model\n",
    "    split_op = train_test_split(\n",
    "        dataset = dataset_op.output)\n",
    "    \n",
    "    # Training the model\n",
    "    model_train_op = train_model(\n",
    "        features = split_op.outputs[\"dataset_train\"]) \n",
    "    \n",
    "    # Evaluating the model\n",
    "    model_evaluation_op = evaluate_model(\n",
    "        model_knn = model_train_op.outputs[\"model\"],\n",
    "        test_set = split_op.outputs[\"dataset_test\"],\n",
    "        thresholds_dict_str = thresholds_dict_str,\n",
    "    )\n",
    "\n",
    "    \n",
    "    with dsl.If(\n",
    "        model_evaluation_op.outputs[\"approval\"]==True,\n",
    "        name=\"approve-model\",\n",
    "    ):\n",
    "           \n",
    "        upload_model_to_gc_op = upload_model_to_gcs(\n",
    "            project_id=project_id,\n",
    "            model_repo=model_repo,\n",
    "            model_name=\"moneyball\",\n",
    "            model = model_train_op.outputs['model']\n",
    "        )    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import compiler\n",
    "\n",
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path = 'ml_moneyball.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "aip.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    ")\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"moneyball-pipeline\",\n",
    "    template_path=\"ml_moneyball.yaml\",\n",
    "    enable_caching=False,\n",
    "    location=REGION,    \n",
    "    parameter_values={\n",
    "            'project_id': PROJECT_ID, # makesure to use your project id\n",
    "            'data_bucket': 'data_de2024_trs',  # makesure to use your data bucket name \n",
    "            'dataset_uri':'gs://data_de2024_trs/baseball_clean.csv',\n",
    "            'model_repo':'models_de2024_trs', # makesure to use your model bucket name \n",
    "            'thresholds_dict_str':'{\"R-squared\":0.7}',\n",
    "            'model_repo_uri':'gs://models_de2024_trs' # makesure to use your model bucket name \n",
    "        }\n",
    ")\n",
    "job.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
